{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48f9f923",
   "metadata": {},
   "source": [
    "# Siamese Network for Semantic Question Similarity\n",
    "\n",
    "## üìã Overview\n",
    "This notebook implements a **Bi-Encoder (Siamese Network)** using **DistilBERT** to detect semantic similarity between questions. Unlike traditional keyword matching, this approach maps questions into a high-dimensional vector space where semantically similar questions are physically closer to one another.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Workflow Steps\n",
    "\n",
    "### 1. Setup & Synthetic Data\n",
    "* **Environment**: Configures PyTorch to utilize GPU (CUDA) for accelerated training.\n",
    "* **Triplet Data**: Generates a synthetic dataset consisting of:\n",
    "    * **Anchor**: The reference question.\n",
    "    * **Positive**: A duplicate/paraphrased version of the anchor.\n",
    "    * **Negative**: A completely unrelated question.\n",
    "\n",
    "### 2. Custom Dataset & Tokenization\n",
    "* **TripletDataset**: A custom PyTorch class that handles on-the-fly tokenization using `distilbert-base-uncased`.\n",
    "* **Padding & Truncation**: Ensures all sequences are normalized to a fixed length for batch processing.\n",
    "\n",
    "### 3. Model Architecture\n",
    "* **Siamese Network**: Implements a Bi-Encoder where three identical DistilBERT models (sharing weights) process the triplet.\n",
    "* **CLS Pooling**: Extracts the `[CLS]` token's hidden state as the definitive 768-dimensional vector representation of the sentence.\n",
    "\n",
    "### 4. Training with Triplet Loss\n",
    "* **Objective**: Uses **Triplet Margin Loss** to minimize the distance between the Anchor and Positive while maximizing the distance between the Anchor and Negative.\n",
    "* **Formula**: $$Loss = \\max(d(Anchor, Positive) - d(Anchor, Negative) + margin, 0)$$\n",
    "* **Optimization**: Employs the `AdamW` optimizer with a linear learning rate.\n",
    "\n",
    "\n",
    "\n",
    "### 5. Semantic Search Inference\n",
    "* **Knowledge Base Indexing**: Pre-calculates and stores embeddings for a \"database\" of known questions.\n",
    "* **Cosine Similarity**: When a user submits a query, it is encoded into a vector and compared against the index using Cosine Similarity.\n",
    "* **Threshold Logic**: Provides a suggested answer if the similarity score exceeds a predefined confidence threshold (e.g., 0.7).\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Key Tech Stack\n",
    "| Category | Tools |\n",
    "| :--- | :--- |\n",
    "| **Deep Learning** | `PyTorch`, `torch.nn` |\n",
    "| **Transformers** | `Hugging Face (transformers)`, `DistilBERT` |\n",
    "| **Analysis** | `pandas`, `numpy`, `scikit-learn` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce3e50f",
   "metadata": {},
   "source": [
    "# 1. Imports & Setup\n",
    "\n",
    "We start by importing PyTorch and the Hugging Face Transformers library. We also set the device to GPU if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a5eadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69496830",
   "metadata": {},
   "source": [
    "# 2. Data Generation (Dummy Data)\n",
    "\n",
    "Instead of downloading a massive dataset, we will create a small synthetic dataset of \"Triplets\".\n",
    "\n",
    "- Anchor: The user's question.\n",
    "\n",
    "- Positive: A different way of asking the same thing (Duplicate).\n",
    "\n",
    "- Negative: A completely unrelated question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "378a11b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 4\n",
      "                               anchor  \\\n",
      "5         How to center a div in CSS?   \n",
      "2     Python list vs tuple difference   \n",
      "4  Best way to learn machine learning   \n",
      "3               How to fix 404 error?   \n",
      "\n",
      "                                      positive  \\\n",
      "5                 CSS centering div techniques   \n",
      "2  Difference between list and tuple in Python   \n",
      "4                     Guide to start ML career   \n",
      "3                 Resolving HTTP 404 not found   \n",
      "\n",
      "                             negative  \n",
      "5  Best way to learn machine learning  \n",
      "2      What is the capital of France?  \n",
      "4               How to fix 404 error?  \n",
      "3     Python list vs tuple difference  \n"
     ]
    }
   ],
   "source": [
    "# Create dummy data\n",
    "data = [\n",
    "    # (Anchor, Positive, Negative)\n",
    "    (\"How do I install PyTorch?\", \"Installation guide for PyTorch\", \"How to make pasta?\"),\n",
    "    (\"What is the capital of France?\", \"France capital city name\", \"How do I install PyTorch?\"),\n",
    "    (\"Python list vs tuple difference\", \"Difference between list and tuple in Python\", \"What is the capital of France?\"),\n",
    "    (\"How to fix 404 error?\", \"Resolving HTTP 404 not found\", \"Python list vs tuple difference\"),\n",
    "    (\"Best way to learn machine learning\", \"Guide to start ML career\", \"How to fix 404 error?\"),\n",
    "    (\"How to center a div in CSS?\", \"CSS centering div techniques\", \"Best way to learn machine learning\"),\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=['anchor', 'positive', 'negative'])\n",
    "\n",
    "# Split into Train and Validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training Samples: {len(train_df)}\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c835837",
   "metadata": {},
   "source": [
    "# 3. Text Preprocessing & Dataset Class\n",
    "\n",
    "We need a custom PyTorch Dataset to handle the tokenization on the fly. We use distilbert-base-uncased because it is fast and effective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619bf59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=32):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        \n",
    "        # Tokenize all three sentences\n",
    "        anchor = self.tokenize(row['anchor'])\n",
    "        positive = self.tokenize(row['positive'])\n",
    "        negative = self.tokenize(row['negative'])\n",
    "        print(f\"Anchor: {row['anchor']}\")\n",
    "        print(f\"Positive: {row['positive']}\")   \n",
    "        print(f\"Negative: {row['negative']}\")\n",
    "        \n",
    "\n",
    "        return {\n",
    "            'anchor_ids': anchor['input_ids'].flatten(),\n",
    "            'anchor_mask': anchor['attention_mask'].flatten(),\n",
    "            'pos_ids': positive['input_ids'].flatten(),\n",
    "            'pos_mask': positive['attention_mask'].flatten(),\n",
    "            'neg_ids': negative['input_ids'].flatten(),\n",
    "            'neg_mask': negative['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TripletDataset(train_df, tokenizer)\n",
    "val_dataset = TripletDataset(val_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083169f",
   "metadata": {},
   "source": [
    "# 4. The Model (Siamese Network)\n",
    "This is the core. We use a Bi-Encoder.\n",
    "\n",
    "1. Pass the sentence through BERT.\n",
    "\n",
    "2. Take the output of the [CLS] token (the first token) as the representation of the entire sentence.\n",
    "\n",
    "3. We do this for Anchor, Positive, and Negative separately (sharing weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9363b96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 137.06it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Load pre-trained BERT model\n",
    "        self.bert = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass input through BERT\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the [CLS] token (first token) hidden state\n",
    "        # Shape: (Batch_Size, Hidden_Dim) -> (2, 768)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        return cls_embedding\n",
    "\n",
    "model = SiameseNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5486ef",
   "metadata": {},
   "source": [
    "# 5. Training Loop\n",
    "\n",
    "We use Triplet Margin Loss.\n",
    "\n",
    "<!-- $$Loss = \\max(d(Anchor, Positive) - d(Anchor, Negative) + margin, 0)$$This forces the Positive to be closer to the Anchor than the Negative is. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8006130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch 1/3 | Loss: 1.1935\n",
      "Epoch 2/3 | Loss: 0.9432\n",
      "Epoch 3/3 | Loss: 0.2229\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 3\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2) # p=2 is Euclidean Distance\n",
    "\n",
    "# Training Function\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        # Move batch to GPU\n",
    "        a_ids = batch['anchor_ids'].to(device)\n",
    "        a_mask = batch['anchor_mask'].to(device)\n",
    "        p_ids = batch['pos_ids'].to(device)\n",
    "        p_mask = batch['pos_mask'].to(device)\n",
    "        n_ids = batch['neg_ids'].to(device)\n",
    "        n_mask = batch['neg_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass (Get embeddings for all 3)\n",
    "        a_emb = model(a_ids, a_mask)\n",
    "        p_emb = model(p_ids, p_mask)\n",
    "        n_emb = model(n_ids, n_mask)\n",
    "\n",
    "        # Calculate Loss\n",
    "        loss = criterion(a_emb, p_emb, n_emb)\n",
    "        \n",
    "        # Backward Pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Run Training\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(epochs):\n",
    "    loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc83663",
   "metadata": {},
   "source": [
    "# 6. Inference Phase\n",
    "Now we act like Quora/Stack Overflow. We have a database of \"`Answered Questions`\". We need to index them.\n",
    "\n",
    "### Step 6a: Create the \"`Database`\" Index \n",
    "We pre-calculate the embeddings for our known questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60e57062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Knowledge Base...\n",
      "Indexing Complete.\n"
     ]
    }
   ],
   "source": [
    "# The \"Database\" of existing questions and their answers\n",
    "knowledge_base = [\n",
    "    {\"id\": 1, \"question\": \"How do I install PyTorch?\", \"answer\": \"Run `pip install torch`.\"},\n",
    "    {\"id\": 2, \"question\": \"What is the capital of France?\", \"answer\": \"The capital is Paris.\"},\n",
    "    {\"id\": 3, \"question\": \"Python list vs tuple difference\", \"answer\": \"Lists are mutable, tuples are immutable.\"},\n",
    "    {\"id\": 4, \"question\": \"How to fix 404 error?\", \"answer\": \"Check your URL or server configuration.\"},\n",
    "]\n",
    "\n",
    "# Function to encode text to vector\n",
    "# --- REPLACEMENT CODE FOR get_embedding FUNCTION ---\n",
    "\n",
    "def get_embedding(text):\n",
    "    model.eval()\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "    \n",
    "    # Move specific tensors to GPU\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # EXPLICITLY pass only the arguments the model expects\n",
    "        # We do NOT pass **inputs here to avoid the 'token_type_ids' error\n",
    "        embedding = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "    return embedding.cpu().numpy()\n",
    "\n",
    "# Index the Knowledge Base\n",
    "print(\"Indexing Knowledge Base...\")\n",
    "kb_embeddings = []\n",
    "for item in knowledge_base:\n",
    "    vec = get_embedding(item['question'])\n",
    "    kb_embeddings.append(vec)\n",
    "\n",
    "# Convert list to numpy array for fast search\n",
    "kb_embeddings = np.vstack(kb_embeddings) # Shape: (4, 768)\n",
    "print(\"Indexing Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341063f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a653227",
   "metadata": {},
   "source": [
    "### Step 6b: The Search Function \n",
    "When a user asks a new question, we convert it to a vector and find the closest vector in our \"Database\" using Cosine Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52c3a32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TESTING THE PIPELINE ---\n",
      "\n",
      "User Query: 'guide to installing torch python library'\n",
      "Found Similar Question (Score: 0.8046): 'Python list vs tuple difference'\n",
      "Suggested Answer: Lists are mutable, tuples are immutable.\n",
      "--------------------------------------------------\n",
      "User Query: 'What is the capital city of France?'\n",
      "Found Similar Question (Score: 0.9975): 'What is the capital of France?'\n",
      "Suggested Answer: The capital is Paris.\n",
      "--------------------------------------------------\n",
      "User Query: 'How do I bake a chocolate cake?'\n",
      "Found Similar Question (Score: 0.9638): 'How do I install PyTorch?'\n",
      "Suggested Answer: Run `pip install torch`.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search_similar_question(user_query, threshold=0.7):\n",
    "    # 1. Convert user query to vector\n",
    "    query_vec = get_embedding(user_query)\n",
    "    \n",
    "    # 2. Calculate similarity with ALL database questions\n",
    "    # (In production, use FAISS for this step)\n",
    "    similarities = cosine_similarity(query_vec, kb_embeddings) # Shape: (1, 4)\n",
    "    \n",
    "    # 3. Find the best match\n",
    "    best_idx = np.argmax(similarities)\n",
    "    best_score = similarities[0, best_idx]\n",
    "    \n",
    "    print(f\"User Query: '{user_query}'\")\n",
    "    \n",
    "    if best_score > threshold:\n",
    "        matched_item = knowledge_base[best_idx]\n",
    "        print(f\"Found Similar Question (Score: {best_score:.4f}): '{matched_item['question']}'\")\n",
    "        print(f\"Suggested Answer: {matched_item['answer']}\")\n",
    "    else:\n",
    "        print(f\"No similar question found. (Best Score: {best_score:.4f})\")\n",
    "        print(\"Please post this as a new question.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Test Cases\n",
    "print(\"\\n--- TESTING THE PIPELINE ---\\n\")\n",
    "\n",
    "# Case 1: Semantic Match (Different wording, same meaning)\n",
    "search_similar_question(\"guide to installing torch python library\")\n",
    "\n",
    "# Case 2: Exact Semantic Match\n",
    "search_similar_question(\"What is the capital city of France?\")\n",
    "\n",
    "# Case 3: Completely New Question\n",
    "search_similar_question(\"How do I bake a chocolate cake?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
