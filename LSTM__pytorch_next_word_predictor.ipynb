{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393483ca",
   "metadata": {},
   "source": [
    "# Next Word Prediction with LSTM\n",
    "\n",
    "## üìã Overview\n",
    "This notebook implements a text generation pipeline using an **LSTM Network**. By training on a custom FAQ dataset, the model learns to predict the most probable next word given a sequence of previous words, effectively acting as an automated \"auto-complete\" for course-related queries.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Workflow Steps\n",
    "\n",
    "### 1. Text Preprocessing & Tokenization\n",
    "* **NLTK Integration**: Uses `word_tokenize` to break the raw document into individual tokens.\n",
    "* **Vocabulary Creation**: Builds a mapping of unique tokens to numerical indices, including an `<unk>` token for out-of-vocabulary terms.\n",
    "* **N-Gram Sequence Generation**: Converts sentences into cumulative sequences (e.g., \"The course fee\" becomes `[The, course]` and `[The, course, fee]`) to teach the model how sentences grow.\n",
    "\n",
    "### 2. Sequence Handling\n",
    "* **Padding**: Uses \"Pre-Padding\" to ensure all input sequences have the same length ($61$ tokens) by adding leading zeros. This is crucial for batch processing in PyTorch.\n",
    "* **X/Y Split**: For every sequence, the last word is treated as the **target (y)** and all preceding words as the **input (X)**.\n",
    "\n",
    "[Image of n-gram sequence generation for next word prediction]\n",
    "\n",
    "### 3. LSTM Model Architecture\n",
    "LSTMs are chosen for their ability to maintain \"long-term memory\" via a cell state, which helps in understanding context in long sentences.\n",
    "* **Embedding Layer**: Learns 100-dimensional dense vectors for each word.\n",
    "* **LSTM Layer**: Processes the sequence and maintains a hidden state of 150 dimensions.\n",
    "* **Linear Output**: A fully connected layer that maps the LSTM output to the total vocabulary size for word selection.\n",
    "\n",
    "[Image of LSTM cell architecture showing forget, input, and output gates]\n",
    "\n",
    "### 4. Training the Generator\n",
    "* **Optimization**: Uses the **Adam Optimizer** and **Cross-Entropy Loss**.\n",
    "* **GPU Acceleration**: Moves the model and tensors to CUDA if available for faster computation over 50 epochs.\n",
    "\n",
    "### 5. Iterative Inference (Text Generation)\n",
    "The prediction function doesn't just predict once; it can be used in a loop:\n",
    "1. Provide a seed phrase (e.g., \"What is the fee\").\n",
    "2. The model predicts the next word (\"for\").\n",
    "3. The new word is appended to the input, and the process repeats.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Key Tech Stack\n",
    "| Category | Tools |\n",
    "| :--- | :--- |\n",
    "| **NLP** | `NLTK (Tokenization)`, `Collections (Counter)` |\n",
    "| **Deep Learning** | `PyTorch (nn.LSTM, nn.Embedding)` |\n",
    "| **Data Processing** | `DataLoader`, `Pre-padding techniques` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "14a3ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6601838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"Q: What is the course fee for the Data Science Mentorship Program?\n",
    "A: The course fee is Rs 799 per month.\n",
    "\n",
    "Q: What is the total duration of the course?\n",
    "A: The total duration of the course is 7 months.\n",
    "\n",
    "Q: What is the total course fee for the full program?\n",
    "A: The total fee is approximately Rs 5600.\n",
    "\n",
    "Q: What modules are covered in the program?\n",
    "A: Python, Data Analysis, SQL, Machine Learning, MLOPs, and case studies are covered.\n",
    "\n",
    "Q: Is Deep Learning part of the curriculum?\n",
    "A: No, Deep Learning is not included.\n",
    "\n",
    "Q: Is NLP included in the program?\n",
    "A: No, NLP is not part of the curriculum.\n",
    "\n",
    "Q: Will recordings be available if I miss a session?\n",
    "A: Yes, all sessions are recorded.\n",
    "\n",
    "Q: Where can I find the class schedule?\n",
    "A: The class schedule is available in the Google Sheet provided.\n",
    "\n",
    "Q: How long does each live session last?\n",
    "A: Each live session lasts around 2 hours.\n",
    "\n",
    "Q: What language is used in the sessions?\n",
    "A: The instructor speaks Hinglish.\n",
    "\n",
    "Q: How will I be notified about upcoming classes?\n",
    "A: You will receive an email before every session.\n",
    "\n",
    "Q: Can non-tech students join the course?\n",
    "A: Yes, students from non-tech backgrounds can join.\n",
    "\n",
    "Q: Can I join the program in the middle?\n",
    "A: Yes, you can join anytime.\n",
    "\n",
    "Q: Will I get access to past lectures if I join late?\n",
    "A: Yes, all past content will be available after payment.\n",
    "\n",
    "Q: Do I need to submit tasks?\n",
    "A: No, you will self-evaluate using provided solutions.\n",
    "\n",
    "Q: Are case studies included in the program?\n",
    "A: Yes, case studies are included.\n",
    "\n",
    "Q: How can I contact the team?\n",
    "A: You can email nitish.campusx@gmail.com.\n",
    "\n",
    "Q: Where should I make payments?\n",
    "A: Payments must be made on the official website.\n",
    "\n",
    "Q: Can I pay the full fee at once?\n",
    "A: No, the program follows a monthly subscription model.\n",
    "\n",
    "Q: What is the validity of the monthly subscription?\n",
    "A: The subscription is valid for 30 days from payment.\n",
    "\n",
    "Q: Is there a refund policy?\n",
    "A: Yes, you get a 7-day refund period.\n",
    "\n",
    "Q: What if I cannot pay from outside India?\n",
    "A: You should contact the team via email.\n",
    "\n",
    "Q: Till when can I watch paid videos?\n",
    "A: You can watch videos while your subscription is valid.\n",
    "\n",
    "Q: Will I get lifetime access after completing the course?\n",
    "A: No, videos are available till Aug 2024 after full payment.\n",
    "\n",
    "Q: Why is lifetime access not provided?\n",
    "A: Because of the low course fee.\n",
    "\n",
    "Q: How can I ask doubts after a session?\n",
    "A: Fill the doubt clearance Google form.\n",
    "\n",
    "Q: Can I ask doubts from past weeks?\n",
    "A: Yes, select past week doubts in the form.\n",
    "\n",
    "Q: What is the criteria for certificate?\n",
    "A: Pay full fee and attempt all assessments.\n",
    "\n",
    "Q: How can I pay earlier month fees if I join late?\n",
    "A: A payment link will be provided in your dashboard.\n",
    "\n",
    "Q: Does placement assistance guarantee a job?\n",
    "A: No, there is no placement guarantee.\n",
    "\n",
    "Q: What is included in placement assistance?\n",
    "A: Portfolio building, soft skills, mentorship, and job strategies.\n",
    "\n",
    "Q: What topics are taught in Python Fundamentals?\n",
    "A: Basics of Python programming and syntax.\n",
    "\n",
    "Q: What Python libraries are taught?\n",
    "A: Libraries for data science such as pandas and numpy.\n",
    "\n",
    "Q: What is covered in Data Analysis?\n",
    "A: Data cleaning, visualization, and exploratory analysis.\n",
    "\n",
    "Q: What is SQL for Data Science?\n",
    "A: SQL concepts for querying databases.\n",
    "\n",
    "Q: What is Maths for Machine Learning?\n",
    "A: Linear algebra, probability, and statistics basics.\n",
    "\n",
    "Q: What ML algorithms are taught?\n",
    "A: Regression, classification, and clustering algorithms.\n",
    "\n",
    "Q: What is Practical Machine Learning?\n",
    "A: Hands-on ML projects and implementations.\n",
    "\n",
    "Q: What is MLOPs?\n",
    "A: Machine learning deployment and operations.\n",
    "\n",
    "Q: Will industry mentors interact with students?\n",
    "A: Yes, sessions with industry mentors are included.\n",
    "\n",
    "Q: Will soft skill sessions be conducted?\n",
    "A: Yes, soft skill sessions are included.\n",
    "\n",
    "Q: Will job hunting strategies be discussed?\n",
    "A: Yes, job hunting strategies will be discussed.\n",
    "\n",
    "Q: Can I access the dashboard after payment?\n",
    "A: Yes, the dashboard becomes available after payment.\n",
    "\n",
    "Q: Can I watch past content immediately after payment?\n",
    "A: Yes, past sessions are unlocked after payment.\n",
    "\n",
    "Q: Is the course suitable for beginners?\n",
    "A: Yes, beginners can join the program.\n",
    "\n",
    "Q: Is the course subscription monthly or yearly?\n",
    "A: The course subscription is monthly.\n",
    "\n",
    "Q: Will there be assessments?\n",
    "A: Yes, assessments are part of the course.\n",
    "\n",
    "Q: Is the course online or offline?\n",
    "A: The course is conducted online.\n",
    "\n",
    "Q: Are doubt sessions one-on-one?\n",
    "A: Yes, 1-on-1 doubt clearance sessions are provided.\n",
    "\n",
    "Q: Is email the only contact method?\n",
    "A: Yes, email is the official contact method.\n",
    "\n",
    "Q: Will the program help build a portfolio?\n",
    "A: Yes, portfolio building sessions are included.\n",
    "\n",
    "Q: Are interview calls guaranteed?\n",
    "A: No, interview calls are not guaranteed.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "50cd3fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c21508df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(document.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "be3752a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0,\n",
       " 'q': 1,\n",
       " ':': 2,\n",
       " 'what': 3,\n",
       " 'is': 4,\n",
       " 'the': 5,\n",
       " 'course': 6,\n",
       " 'fee': 7,\n",
       " 'for': 8,\n",
       " 'data': 9,\n",
       " 'science': 10,\n",
       " 'mentorship': 11,\n",
       " 'program': 12,\n",
       " '?': 13,\n",
       " 'a': 14,\n",
       " 'rs': 15,\n",
       " '799': 16,\n",
       " 'per': 17,\n",
       " 'month': 18,\n",
       " '.': 19,\n",
       " 'total': 20,\n",
       " 'duration': 21,\n",
       " 'of': 22,\n",
       " '7': 23,\n",
       " 'months': 24,\n",
       " 'full': 25,\n",
       " 'approximately': 26,\n",
       " '5600.': 27,\n",
       " 'modules': 28,\n",
       " 'are': 29,\n",
       " 'covered': 30,\n",
       " 'in': 31,\n",
       " 'python': 32,\n",
       " ',': 33,\n",
       " 'analysis': 34,\n",
       " 'sql': 35,\n",
       " 'machine': 36,\n",
       " 'learning': 37,\n",
       " 'mlops': 38,\n",
       " 'and': 39,\n",
       " 'case': 40,\n",
       " 'studies': 41,\n",
       " 'deep': 42,\n",
       " 'part': 43,\n",
       " 'curriculum': 44,\n",
       " 'no': 45,\n",
       " 'not': 46,\n",
       " 'included': 47,\n",
       " 'nlp': 48,\n",
       " 'will': 49,\n",
       " 'recordings': 50,\n",
       " 'be': 51,\n",
       " 'available': 52,\n",
       " 'if': 53,\n",
       " 'i': 54,\n",
       " 'miss': 55,\n",
       " 'session': 56,\n",
       " 'yes': 57,\n",
       " 'all': 58,\n",
       " 'sessions': 59,\n",
       " 'recorded': 60,\n",
       " 'where': 61,\n",
       " 'can': 62,\n",
       " 'find': 63,\n",
       " 'class': 64,\n",
       " 'schedule': 65,\n",
       " 'google': 66,\n",
       " 'sheet': 67,\n",
       " 'provided': 68,\n",
       " 'how': 69,\n",
       " 'long': 70,\n",
       " 'does': 71,\n",
       " 'each': 72,\n",
       " 'live': 73,\n",
       " 'last': 74,\n",
       " 'lasts': 75,\n",
       " 'around': 76,\n",
       " '2': 77,\n",
       " 'hours': 78,\n",
       " 'language': 79,\n",
       " 'used': 80,\n",
       " 'instructor': 81,\n",
       " 'speaks': 82,\n",
       " 'hinglish': 83,\n",
       " 'notified': 84,\n",
       " 'about': 85,\n",
       " 'upcoming': 86,\n",
       " 'classes': 87,\n",
       " 'you': 88,\n",
       " 'receive': 89,\n",
       " 'an': 90,\n",
       " 'email': 91,\n",
       " 'before': 92,\n",
       " 'every': 93,\n",
       " 'non-tech': 94,\n",
       " 'students': 95,\n",
       " 'join': 96,\n",
       " 'from': 97,\n",
       " 'backgrounds': 98,\n",
       " 'middle': 99,\n",
       " 'anytime': 100,\n",
       " 'get': 101,\n",
       " 'access': 102,\n",
       " 'to': 103,\n",
       " 'past': 104,\n",
       " 'lectures': 105,\n",
       " 'late': 106,\n",
       " 'content': 107,\n",
       " 'after': 108,\n",
       " 'payment': 109,\n",
       " 'do': 110,\n",
       " 'need': 111,\n",
       " 'submit': 112,\n",
       " 'tasks': 113,\n",
       " 'self-evaluate': 114,\n",
       " 'using': 115,\n",
       " 'solutions': 116,\n",
       " 'contact': 117,\n",
       " 'team': 118,\n",
       " 'nitish.campusx': 119,\n",
       " '@': 120,\n",
       " 'gmail.com': 121,\n",
       " 'should': 122,\n",
       " 'make': 123,\n",
       " 'payments': 124,\n",
       " 'must': 125,\n",
       " 'made': 126,\n",
       " 'on': 127,\n",
       " 'official': 128,\n",
       " 'website': 129,\n",
       " 'pay': 130,\n",
       " 'at': 131,\n",
       " 'once': 132,\n",
       " 'follows': 133,\n",
       " 'monthly': 134,\n",
       " 'subscription': 135,\n",
       " 'model': 136,\n",
       " 'validity': 137,\n",
       " 'valid': 138,\n",
       " '30': 139,\n",
       " 'days': 140,\n",
       " 'there': 141,\n",
       " 'refund': 142,\n",
       " 'policy': 143,\n",
       " '7-day': 144,\n",
       " 'period': 145,\n",
       " 'outside': 146,\n",
       " 'india': 147,\n",
       " 'via': 148,\n",
       " 'till': 149,\n",
       " 'when': 150,\n",
       " 'watch': 151,\n",
       " 'paid': 152,\n",
       " 'videos': 153,\n",
       " 'while': 154,\n",
       " 'your': 155,\n",
       " 'lifetime': 156,\n",
       " 'completing': 157,\n",
       " 'aug': 158,\n",
       " '2024': 159,\n",
       " 'why': 160,\n",
       " 'because': 161,\n",
       " 'low': 162,\n",
       " 'ask': 163,\n",
       " 'doubts': 164,\n",
       " 'fill': 165,\n",
       " 'doubt': 166,\n",
       " 'clearance': 167,\n",
       " 'form': 168,\n",
       " 'weeks': 169,\n",
       " 'select': 170,\n",
       " 'week': 171,\n",
       " 'criteria': 172,\n",
       " 'certificate': 173,\n",
       " 'attempt': 174,\n",
       " 'assessments': 175,\n",
       " 'earlier': 176,\n",
       " 'fees': 177,\n",
       " 'link': 178,\n",
       " 'dashboard': 179,\n",
       " 'placement': 180,\n",
       " 'assistance': 181,\n",
       " 'guarantee': 182,\n",
       " 'job': 183,\n",
       " 'portfolio': 184,\n",
       " 'building': 185,\n",
       " 'soft': 186,\n",
       " 'skills': 187,\n",
       " 'strategies': 188,\n",
       " 'topics': 189,\n",
       " 'taught': 190,\n",
       " 'fundamentals': 191,\n",
       " 'basics': 192,\n",
       " 'programming': 193,\n",
       " 'syntax': 194,\n",
       " 'libraries': 195,\n",
       " 'such': 196,\n",
       " 'as': 197,\n",
       " 'pandas': 198,\n",
       " 'numpy': 199,\n",
       " 'cleaning': 200,\n",
       " 'visualization': 201,\n",
       " 'exploratory': 202,\n",
       " 'concepts': 203,\n",
       " 'querying': 204,\n",
       " 'databases': 205,\n",
       " 'maths': 206,\n",
       " 'linear': 207,\n",
       " 'algebra': 208,\n",
       " 'probability': 209,\n",
       " 'statistics': 210,\n",
       " 'ml': 211,\n",
       " 'algorithms': 212,\n",
       " 'regression': 213,\n",
       " 'classification': 214,\n",
       " 'clustering': 215,\n",
       " 'practical': 216,\n",
       " 'hands-on': 217,\n",
       " 'projects': 218,\n",
       " 'implementations': 219,\n",
       " 'deployment': 220,\n",
       " 'operations': 221,\n",
       " 'industry': 222,\n",
       " 'mentors': 223,\n",
       " 'interact': 224,\n",
       " 'with': 225,\n",
       " 'skill': 226,\n",
       " 'conducted': 227,\n",
       " 'hunting': 228,\n",
       " 'discussed': 229,\n",
       " 'becomes': 230,\n",
       " 'immediately': 231,\n",
       " 'unlocked': 232,\n",
       " 'suitable': 233,\n",
       " 'beginners': 234,\n",
       " 'or': 235,\n",
       " 'yearly': 236,\n",
       " 'online': 237,\n",
       " 'offline': 238,\n",
       " 'one-on-one': 239,\n",
       " '1-on-1': 240,\n",
       " 'only': 241,\n",
       " 'method': 242,\n",
       " 'help': 243,\n",
       " 'build': 244,\n",
       " 'interview': 245,\n",
       " 'calls': 246,\n",
       " 'guaranteed': 247}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {'<unk>':0}\n",
    "\n",
    "for token in Counter(tokens).keys():\n",
    "  if token not in vocab:\n",
    "    vocab[token] = len(vocab)\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bc71052a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "832c643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = document.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc132f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "70d31e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_indices(sentence_tokens, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in sentence_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6e6a88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_numerical_sentences = []\n",
    "\n",
    "for sentence in input_sentences:\n",
    "  input_numerical_sentences.append(text_to_indices(word_tokenize(sentence.lower()), vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c258b715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7, 8, 5, 9, 10, 11, 12, 13],\n",
       " [14, 2, 5, 6, 7, 4, 15, 16, 17, 18, 19],\n",
       " [],\n",
       " [1, 2, 3, 4, 5, 20, 21, 22, 5, 6, 13],\n",
       " [14, 2, 5, 20, 21, 22, 5, 6, 4, 23, 24, 19]]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_numerical_sentences)\n",
    "input_numerical_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "374bdec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sequence = []\n",
    "for sentence in input_numerical_sentences:\n",
    "# we did this because it will help us to create x and y pairs for training the model. For example if we have a sentence like [1,2,3,4] then we will create training sequences like [1,2], [1,2,3], [1,2,3,4] and the corresponding target will be 3,4 and <eos> respectively. This way we can train our model to predict the next word in the sequence.\n",
    "  for i in range(1, len(sentence)):\n",
    "    training_sequence.append(sentence[:i+1]) #i+1 because we want to include the word at index i as well in the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "200cbd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5, 6]]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_sequence)\n",
    "training_sequence[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3bb82115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_list = []\n",
    "\n",
    "for sequence in training_sequence:\n",
    "  len_list.append(len(sequence))\n",
    "\n",
    "max(len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7005c720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sequence[0]\n",
    "# for equal length of each sentences in training we will use padding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "66f1f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_training_sequence = []\n",
    "for sequence in training_sequence:\n",
    "\n",
    "  padded_training_sequence.append([0]*(max(len_list) - len(sequence)) + sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "656f3118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_training_sequence[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "90f075d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 5, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "print(padded_training_sequence[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4f653eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(padded_training_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dfd00b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert this into a tensor \n",
    "padded_training_sequence = torch.tensor(padded_training_sequence,dtype=torch.long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "455943d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate x and y\n",
    "X = padded_training_sequence[:, :-1]\n",
    "y = padded_training_sequence[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "928b24ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,  ...,   0,   0,   1],\n",
       "        [  0,   0,   0,  ...,   0,   1,   2],\n",
       "        [  0,   0,   0,  ...,   1,   2,   3],\n",
       "        ...,\n",
       "        [  0,   0,   0,  ..., 245, 246,  29],\n",
       "        [  0,   0,   0,  ..., 246,  29,  46],\n",
       "        [  0,   0,   0,  ...,  29,  46, 247]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1696d947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,   3,   4,   5,   6,   7,   8,   5,   9,  10,  11,  12,  13,   2,\n",
       "          5,   6,   7,   4,  15,  16,  17,  18,  19,   2,   3,   4,   5,  20,\n",
       "         21,  22,   5,   6,  13,   2,   5,  20,  21,  22,   5,   6,   4,  23,\n",
       "         24,  19,   2,   3,   4,   5,  20,   6,   7,   8,   5,  25,  12,  13,\n",
       "          2,   5,  20,   7,   4,  26,  15,   0,  19,   2,   3,  28,  29,  30,\n",
       "         31,   5,  12,  13,   2,  32,  33,   9,  34,  33,  35,  33,  36,  37,\n",
       "         33,  38,  33,  39,  40,  41,  29,  30,  19,   2,   4,  42,  37,  43,\n",
       "         22,   5,  44,  13,   2,  45,  33,  42,  37,   4,  46,  47,  19,   2,\n",
       "          4,  48,  47,  31,   5,  12,  13,   2,  45,  33,  48,   4,  46,  43,\n",
       "         22,   5,  44,  19,   2,  49,  50,  51,  52,  53,  54,  55,  14,  56,\n",
       "         13,   2,  57,  33,  58,  59,  29,  60,  19,   2,  61,  62,  54,  63,\n",
       "          5,  64,  65,  13,   2,   5,  64,  65,   4,  52,  31,   5,  66,  67,\n",
       "         68,  19,   2,  69,  70,  71,  72,  73,  56,  74,  13,   2,  72,  73,\n",
       "         56,  75,  76,  77,  78,  19,   2,   3,  79,   4,  80,  31,   5,  59,\n",
       "         13,   2,   5,  81,  82,  83,  19,   2,  69,  49,  54,  51,  84,  85,\n",
       "         86,  87,  13,   2,  88,  49,  89,  90,  91,  92,  93,  56,  19,   2,\n",
       "         62,  94,  95,  96,   5,   6,  13,   2,  57,  33,  95,  97,  94,  98,\n",
       "         62,  96,  19,   2,  62,  54,  96,   5,  12,  31,   5,  99,  13,   2,\n",
       "         57,  33,  88,  62,  96, 100,  19,   2,  49,  54, 101, 102, 103, 104,\n",
       "        105,  53,  54,  96, 106,  13,   2,  57,  33,  58, 104, 107,  49,  51,\n",
       "         52, 108, 109,  19,   2, 110,  54, 111, 103, 112, 113,  13,   2,  45,\n",
       "         33,  88,  49, 114, 115,  68, 116,  19,   2,  29,  40,  41,  47,  31,\n",
       "          5,  12,  13,   2,  57,  33,  40,  41,  29,  47,  19,   2,  69,  62,\n",
       "         54, 117,   5, 118,  13,   2,  88,  62,  91, 119, 120, 121,  19,   2,\n",
       "         61, 122,  54, 123, 124,  13,   2, 124, 125,  51, 126, 127,   5, 128,\n",
       "        129,  19,   2,  62,  54, 130,   5,  25,   7, 131, 132,  13,   2,  45,\n",
       "         33,   5,  12, 133,  14, 134, 135, 136,  19,   2,   3,   4,   5, 137,\n",
       "         22,   5, 134, 135,  13,   2,   5, 135,   4, 138,   8, 139, 140,  97,\n",
       "        109,  19,   2,   4, 141,  14, 142, 143,  13,   2,  57,  33,  88, 101,\n",
       "         14, 144, 142, 145,  19,   2,   3,  53,  54,  62,  46, 130,  97, 146,\n",
       "        147,  13,   2,  88, 122, 117,   5, 118, 148,  91,  19,   2, 149, 150,\n",
       "         62,  54, 151, 152, 153,  13,   2,  88,  62, 151, 153, 154, 155, 135,\n",
       "          4, 138,  19,   2,  49,  54, 101, 156, 102, 108, 157,   5,   6,  13,\n",
       "          2,  45,  33, 153,  29,  52, 149, 158, 159, 108,  25, 109,  19,   2,\n",
       "        160,   4, 156, 102,  46,  68,  13,   2, 161,  22,   5, 162,   6,   7,\n",
       "         19,   2,  69,  62,  54, 163, 164, 108,  14,  56,  13,   2, 165,   5,\n",
       "        166, 167,  66, 168,  19,   2,  62,  54, 163, 164,  97, 104, 169,  13,\n",
       "          2,  57,  33, 170, 104, 171, 164,  31,   5, 168,  19,   2,   3,   4,\n",
       "          5, 172,   8, 173,  13,   2, 130,  25,   7,  39, 174,  58, 175,  19,\n",
       "          2,  69,  62,  54, 130, 176,  18, 177,  53,  54,  96, 106,  13,   2,\n",
       "         14, 109, 178,  49,  51,  68,  31, 155, 179,  19,   2,  71, 180, 181,\n",
       "        182,  14, 183,  13,   2,  45,  33, 141,   4,  45, 180, 182,  19,   2,\n",
       "          3,   4,  47,  31, 180, 181,  13,   2, 184, 185,  33, 186, 187,  33,\n",
       "         11,  33,  39, 183, 188,  19,   2,   3, 189,  29, 190,  31,  32, 191,\n",
       "         13,   2, 192,  22,  32, 193,  39, 194,  19,   2,   3,  32, 195,  29,\n",
       "        190,  13,   2, 195,   8,   9,  10, 196, 197, 198,  39, 199,  19,   2,\n",
       "          3,   4,  30,  31,   9,  34,  13,   2,   9, 200,  33, 201,  33,  39,\n",
       "        202,  34,  19,   2,   3,   4,  35,   8,   9,  10,  13,   2,  35, 203,\n",
       "          8, 204, 205,  19,   2,   3,   4, 206,   8,  36,  37,  13,   2, 207,\n",
       "        208,  33, 209,  33,  39, 210, 192,  19,   2,   3, 211, 212,  29, 190,\n",
       "         13,   2, 213,  33, 214,  33,  39, 215, 212,  19,   2,   3,   4, 216,\n",
       "         36,  37,  13,   2, 217, 211, 218,  39, 219,  19,   2,   3,   4,  38,\n",
       "         13,   2,  36,  37, 220,  39, 221,  19,   2,  49, 222, 223, 224, 225,\n",
       "         95,  13,   2,  57,  33,  59, 225, 222, 223,  29,  47,  19,   2,  49,\n",
       "        186, 226,  59,  51, 227,  13,   2,  57,  33, 186, 226,  59,  29,  47,\n",
       "         19,   2,  49, 183, 228, 188,  51, 229,  13,   2,  57,  33, 183, 228,\n",
       "        188,  49,  51, 229,  19,   2,  62,  54, 102,   5, 179, 108, 109,  13,\n",
       "          2,  57,  33,   5, 179, 230,  52, 108, 109,  19,   2,  62,  54, 151,\n",
       "        104, 107, 231, 108, 109,  13,   2,  57,  33, 104,  59,  29, 232, 108,\n",
       "        109,  19,   2,   4,   5,   6, 233,   8, 234,  13,   2,  57,  33, 234,\n",
       "         62,  96,   5,  12,  19,   2,   4,   5,   6, 135, 134, 235, 236,  13,\n",
       "          2,   5,   6, 135,   4, 134,  19,   2,  49, 141,  51, 175,  13,   2,\n",
       "         57,  33, 175,  29,  43,  22,   5,   6,  19,   2,   4,   5,   6, 237,\n",
       "        235, 238,  13,   2,   5,   6,   4, 227, 237,  19,   2,  29, 166,  59,\n",
       "        239,  13,   2,  57,  33, 240, 166, 167,  59,  29,  68,  19,   2,   4,\n",
       "         91,   5, 241, 117, 242,  13,   2,  57,  33,  91,   4,   5, 128, 117,\n",
       "        242,  19,   2,  49,   5,  12, 243, 244,  14, 184,  13,   2,  57,  33,\n",
       "        184, 185,  59,  29,  47,  19,   2,  29, 245, 246, 247,  13,   2,  45,\n",
       "         33, 245, 246,  29,  46, 247,  19])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0652e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.X.shape[0]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "661c55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "26fb50cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "959"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b8534205",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1d75bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "\n",
    "  def __init__(self, vocab_size):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, 100)\n",
    "    self.lstm = nn.LSTM(100, 150, batch_first=True)\n",
    "    self.fc = nn.Linear(150, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    embedded = self.embedding(x)\n",
    "    intermediate_hidden_states, (final_hidden_state, final_cell_state) = self.lstm(embedded)\n",
    "    output = self.fc(final_hidden_state.squeeze(0))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5a80ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "35cd22e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a6dd3968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embedding): Embedding(248, 100)\n",
       "  (lstm): LSTM(100, 150, batch_first=True)\n",
       "  (fc): Linear(in_features=150, out_features=248, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "545be363",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9241d115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 149.9856\n",
      "Epoch: 2, Loss: 122.4637\n",
      "Epoch: 3, Loss: 109.9638\n",
      "Epoch: 4, Loss: 99.7200\n",
      "Epoch: 5, Loss: 90.6483\n",
      "Epoch: 6, Loss: 82.2432\n",
      "Epoch: 7, Loss: 74.9682\n",
      "Epoch: 8, Loss: 68.1077\n",
      "Epoch: 9, Loss: 61.7338\n",
      "Epoch: 10, Loss: 56.0683\n",
      "Epoch: 11, Loss: 51.1080\n",
      "Epoch: 12, Loss: 46.2950\n",
      "Epoch: 13, Loss: 42.3509\n",
      "Epoch: 14, Loss: 38.5963\n",
      "Epoch: 15, Loss: 35.3854\n",
      "Epoch: 16, Loss: 32.4524\n",
      "Epoch: 17, Loss: 30.1617\n",
      "Epoch: 18, Loss: 28.0440\n",
      "Epoch: 19, Loss: 26.2975\n",
      "Epoch: 20, Loss: 24.9816\n",
      "Epoch: 21, Loss: 23.5191\n",
      "Epoch: 22, Loss: 22.4846\n",
      "Epoch: 23, Loss: 21.5051\n",
      "Epoch: 24, Loss: 20.8324\n",
      "Epoch: 25, Loss: 20.1089\n",
      "Epoch: 26, Loss: 19.4302\n",
      "Epoch: 27, Loss: 18.9418\n",
      "Epoch: 28, Loss: 18.4199\n",
      "Epoch: 29, Loss: 18.1243\n",
      "Epoch: 30, Loss: 17.6575\n",
      "Epoch: 31, Loss: 17.4511\n",
      "Epoch: 32, Loss: 17.2155\n",
      "Epoch: 33, Loss: 16.9507\n",
      "Epoch: 34, Loss: 16.6268\n",
      "Epoch: 35, Loss: 16.4645\n",
      "Epoch: 36, Loss: 16.2904\n",
      "Epoch: 37, Loss: 16.1996\n",
      "Epoch: 38, Loss: 15.9363\n",
      "Epoch: 39, Loss: 15.7993\n",
      "Epoch: 40, Loss: 15.7430\n",
      "Epoch: 41, Loss: 15.5300\n",
      "Epoch: 42, Loss: 15.5345\n",
      "Epoch: 43, Loss: 15.4732\n",
      "Epoch: 44, Loss: 15.3242\n",
      "Epoch: 45, Loss: 15.2259\n",
      "Epoch: 46, Loss: 15.2575\n",
      "Epoch: 47, Loss: 15.0684\n",
      "Epoch: 48, Loss: 14.9592\n",
      "Epoch: 49, Loss: 14.9745\n",
      "Epoch: 50, Loss: 14.8734\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "  total_loss = 0\n",
    "\n",
    "  for batch_x, batch_y in dataloader:\n",
    "\n",
    "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(batch_x)\n",
    "\n",
    "    loss = criterion(output, batch_y)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "  print(f\"Epoch: {epoch + 1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "888371e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "\n",
    "def prediction(model, vocab, text):\n",
    "\n",
    "  # tokenize\n",
    "  tokenized_text = word_tokenize(text.lower())\n",
    "\n",
    "  # text -> numerical indices\n",
    "  numerical_text = text_to_indices(tokenized_text, vocab)\n",
    "\n",
    "  # padding\n",
    "  padded_text = torch.tensor([0] * (61 - len(numerical_text)) + numerical_text, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "  # send to model\n",
    "  output = model(padded_text.to(device))\n",
    "\n",
    "  # predicted index\n",
    "  value, index = torch.max(output, dim=1)\n",
    "\n",
    "  # merge with text\n",
    "  return text + \" \" + list(vocab.keys())[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3a07da1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The course follows a monthly subscription'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction(model, vocab, \"The course follows a monthly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "08932167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the fee for course is the course fee for the data science mentorship program\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_tokens = 10\n",
    "input_text = \"What is the fee for course\"\n",
    "#no duplicate answer\n",
    "\n",
    "for i in range(num_tokens):\n",
    "  output_text = prediction(model, vocab, input_text)\n",
    "#   print(output_text)\n",
    "  input_text = output_text\n",
    "  time.sleep(0.5)\n",
    "\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6fca1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
