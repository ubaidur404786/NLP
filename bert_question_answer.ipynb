{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad826f7",
   "metadata": {},
   "source": [
    "# Extractive QA with RoBERTa & Hugging Face\n",
    "\n",
    "## üìã Overview\n",
    "This notebook implements an **Extractive Question Answering** system. Unlike generative models that \"write\" an answer, this system identifies the exact span of text (start and end indices) within a provided context that best answers a given question. It utilizes the `RoBERTa` architecture fine-tuned on the SQuAD 2.0 dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Workflow Steps\n",
    "\n",
    "### 1. Model Selection & Setup\n",
    "* **Model**: Uses `deepset/roberta-base-squad2`, a robust transformer model optimized specifically for understanding context and extracting answers.\n",
    "* **Hugging Face Hub**: Demonstrates how to load models and tokenizers seamlessly using `AutoModelForQuestionAnswering`.\n",
    "\n",
    "### 2. Manual Inference (Low-Level)\n",
    "* **Tokenization**: Questions and contexts are paired and converted into input tensors.\n",
    "* **Logit Analysis**: The model outputs `start_logits` and `end_logits`.\n",
    "    * The code uses `torch.argmax` to find the most probable start and end positions of the answer within the context.\n",
    "* **Decoding**: Converts the numerical token IDs back into human-readable text.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Pipeline Inference (High-Level)\n",
    "* **Abstraction**: Implements the Hugging Face `pipeline` API, which encapsulates tokenization, forward pass, and decoding into a single line of code.\n",
    "* **Efficiency**: This method is preferred for production as it handles all the heavy lifting (like sliding windows for long contexts) automatically.\n",
    "\n",
    "### 4. Architecture Insight: BERT & Context\n",
    "* The notebook includes a practical example of BERT's **Bidirectional** approach, allowing it to look at the words both to the left and the right of a token to understand its full meaning.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Key Tech Stack\n",
    "| Category | Tools |\n",
    "| :--- | :--- |\n",
    "| **Model Hub** | `Hugging Face Transformers` |\n",
    "| **Model** | `RoBERTa (SQuAD 2.0)` |\n",
    "| **Tensors** | `PyTorch (torch)` |\n",
    "| **Pipeline** | `transformers.pipeline` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "667658ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForQuestionAnswering,AutoTokenizer,pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a061a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_input=[{\n",
    "'question':'How to learn NLP?','context':'NLP stands for Natural Language Processing.'\n",
    "' It is a field of artificial intelligence that enables computers to understand and process human language. To learn NLP, you can start with online courses, tutorials, and books on the subject. It is also helpful to practice by working on projects and using NLP libraries such as NLTK, spaCy, and Hugging Face Transformers.'\n",
    "\n",
    "},\n",
    "{'question':\"Explain Architecture of BERT?\",'context':'BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model that uses a bidirectional approach to understand the context of words in a sentence.'\n",
    "' It consists of multiple layers of transformers, which are attention-based mechanisms that allow the model to focus on different parts of the input text. BERT is pre-trained on a large corpus of text and can be fine-tuned for various '\n",
    "'NLP tasks such as question answering, sentiment analysis, and named entity recognition.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175eb786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select from the hub of hugging face\n",
    "model_name=\"deepset/roberta-base-squad2\" \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d40170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "model=AutoModelForQuestionAnswering.from_pretrained(model_name) \n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aadd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "inputs0=tokenizer(QA_input[0]['question'],QA_input[0]['context'],return_tensors='pt')\n",
    "\n",
    "outputs0=model(**inputs0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf56fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1=tokenizer(QA_input[1]['question'],QA_input[1]['context'],return_tensors='pt')\n",
    "outputs1=model(**input1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d668bb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.8542, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48626aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_start_idx=torch.argmax(outputs0.start_logits)\n",
    "answer_end_idx=torch.argmax(outputs0.end_logits)\n",
    "\n",
    "\n",
    "answer_start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83eec864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.8542, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs0.start_logits[0][43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c10d0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question How to learn NLP? answer:  online courses, tutorials, and books on the subject\n"
     ]
    }
   ],
   "source": [
    "answer_tokens=inputs0.input_ids[0,answer_start_idx:answer_end_idx+1]\n",
    "answer=tokenizer.decode(answer_tokens)\n",
    "print(\"question {} answer: {}\".format(QA_input[0]['question'],answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e37e0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question Explain Architecture of BERT? answer:  multiple layers of transformers\n"
     ]
    }
   ],
   "source": [
    "#now for the second question\n",
    "answer_start_idx1=torch.argmax(outputs1.start_logits) \n",
    "answer_end_idx1=torch.argmax(outputs1.end_logits) \n",
    "answer_tokens1=input1.input_ids[0,answer_start_idx1:answer_end_idx1+1] \n",
    "answer1=tokenizer.decode(answer_tokens1)\n",
    "print(\"question {} answer: {}\".format(QA_input[1]['question'],answer1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc50a6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question How to learn NLP? answer: online courses, tutorials, and books on the subject\n"
     ]
    }
   ],
   "source": [
    "#method 2 using pipeline\n",
    "qa=pipeline('question-answering',model=model_name,tokenizer=model_name)\n",
    "result=qa(question=QA_input[0]['question'],context=QA_input[0]['context']) \n",
    "print(\"question {} answer: {}\".format(QA_input[0]['question'],result['answer'])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "248fbd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question Explain Architecture of BERT? answer: multiple layers of transformers\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result1=qa(question=QA_input[1]['question'],context=QA_input[1]['context']) \n",
    "print(\"question {} answer: {}\".format(QA_input[1]['question'],result1['answer']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05630d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
